"""
Batch GEO V2 å»ºè®®æ”¶é›†å™¨
ä½¿ç”¨ä¸¤é˜¶æ®µåˆ†æï¼ˆè¯Šæ–­ + ç­–ç•¥é€‰æ‹©ï¼‰æ”¶é›†ä¼˜åŒ–å»ºè®®

V2.1 æ›´æ–°ï¼š
- å®ç°ä¸ GEO Agent å®Œå…¨ä¸€è‡´çš„ retry éªŒè¯æœºåˆ¶
- åœ¨ä¸´æ—¶ç»“æ„ä¸Šæµ‹è¯•ä¿®æ”¹ï¼Œæ£€æŸ¥æ˜¯å¦ç”Ÿæ•ˆ
- åªè¿”å›æœ€ç»ˆç”Ÿæ•ˆçš„å»ºè®®
"""
import asyncio
import logging
import sys
import uuid
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

# è®¾ç½®è·¯å¾„
REPO_ROOT = Path(__file__).resolve().parents[1]
GEO_AGENT_ROOT = REPO_ROOT / "geo_agent"
if str(GEO_AGENT_ROOT) not in sys.path:
    sys.path.insert(0, str(GEO_AGENT_ROOT))
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(1, str(REPO_ROOT))

from geo_agent.core.models import CitationCheckResult
from geo_agent.core.memory import parse_tool_output
from geo_agent.utils.structural_parser import ContentChunk, StructuralHtmlParser
from geo_agent.tools import registry

# å¤ç”¨ geo_agent æ ¸å¿ƒæ¨¡å—ï¼ˆä¸ geo_agent å¯¹é½ï¼‰
from geo_agent.agent.content_auditor import audit_content_truncation, TruncationAuditResult
from geo_agent.agent.policy_engine import PolicyEngine as GeoAgentPolicyEngine
from geo_agent.core.telemetry import (
    TelemetryStore,
    FailureCategory,
    ToolInvocationSpan,
    IterationMetrics,
    ToolOutcome,
    compute_args_hash,
)

from .failure_analysis import analyze_failure_async, DiagnosisResult, regenerate_tool_args_async
from .memory_manager import OptimizationMemoryV2, PolicyEngine as BatchPolicyEngine, HistoryManagerV2
from .models import DiagnosisInfo, QueryResultV2, SuggestionV2
from .tool_executor import ToolExecutor
from .citation_checker import compute_geo_score, GEOScoreInfo

logger = logging.getLogger(__name__)


class SuggestionCollectorV2:
    """
    V2 å»ºè®®æ”¶é›†å™¨

    ä½¿ç”¨ä¸¤é˜¶æ®µåˆ†æï¼š
    1. è¯Šæ–­ï¼ˆDiagnoseï¼‰- è¯†åˆ«å¤±è´¥æ ¹å› 
    2. ç­–ç•¥é€‰æ‹©ï¼ˆSelect Tool Strategyï¼‰- é€‰æ‹©æœ€ä½³å·¥å…·

    æ”¯æŒï¼š
    - å¹¶è¡Œå¤„ç†å¤šä¸ª query
    - æ¯ä¸ª query å¤šæ¬¡é‡è¯•
    - è¯Šæ–­ä¿¡æ¯è®°å½•
    - ç­–ç•¥æ³¨å…¥
    """

    def __init__(
        self,
        llm,
        generator: Any,  # AsyncInContextGenerator
        chunks: List[ContentChunk],
        core_ideas: Dict[int, str],
        history_context: str = "",
        chunks_per_orchestra: int = 2,
        history_manager: Optional[HistoryManagerV2] = None,
        enable_policy_injection: bool = True,
        enable_memory: bool = True,
        enable_history: bool = True,
        excluded_tools: Optional[List[str]] = None,
    ):
        self.llm = llm
        self.generator = generator
        self.chunks = chunks
        self.core_ideas = core_ideas
        self.history_context = history_context
        self.chunks_per_orchestra = chunks_per_orchestra
        self.tool_executor = ToolExecutor()
        self.history_manager = history_manager or HistoryManagerV2()
        self.enable_policy_injection = enable_policy_injection
        self.enable_memory = enable_memory
        self.enable_history = enable_history
        self.excluded_tools = excluded_tools or []

        # æ¯ä¸ª query çš„å†…å­˜ï¼ˆç”¨äºè¿½è¸ªé‡è¯•ï¼‰
        self._query_memories: Dict[str, OptimizationMemoryV2] = {}

    def _get_chunks_summary(self) -> str:
        """ç”Ÿæˆ chunks æ‘˜è¦ï¼ˆç”¨äº LLM é€‰æ‹©ç›®æ ‡ chunkï¼‰"""
        summary_parts = []
        for i, chunk in enumerate(self.chunks):
            text_preview = chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text
            summary_parts.append(f"[CHUNK {i}]: {text_preview}")
        return "\n\n".join(summary_parts)

    def _format_indexed_content(self) -> str:
        """ç”Ÿæˆå¸¦ç´¢å¼•çš„å®Œæ•´å†…å®¹"""
        return "\n\n".join(
            [f">> [CHUNK_ID: {i}]\n{chunk.text}" for i, chunk in enumerate(self.chunks)]
        )

    def _sanitize_for_logging(self, tool_args: Dict[str, Any]) -> Dict[str, Any]:
        """è„±æ•å·¥å…·å‚æ•°ï¼Œç§»é™¤è¿‡é•¿å†…å®¹ï¼ˆä¸ geo_agent å¯¹é½ï¼‰"""
        sanitized = {}
        for key, value in tool_args.items():
            if key in ["target_content", "context_before", "context_after"]:
                sanitized[key] = f"<{len(str(value))} chars>" if value else ""
            elif key == "previous_modifications":
                lines = str(value).strip().split('\n') if value else []
                sanitized[key] = f"<{len(lines)} rules>"
            else:
                sanitized[key] = value
        return sanitized

    def _str_to_failure_category(self, diagnosis_str: str) -> FailureCategory:
        """å°†å­—ç¬¦ä¸²è¯Šæ–­è½¬æ¢ä¸º FailureCategory æšä¸¾"""
        try:
            return FailureCategory(diagnosis_str.lower())
        except ValueError:
            return FailureCategory.UNKNOWN

    def _get_orchestra_core_idea(self, chunk_index: int) -> str:
        """è·å– chunk æ‰€å± orchestra çš„æ ¸å¿ƒæ€æƒ³"""
        orchestra_id = chunk_index // self.chunks_per_orchestra
        return self.core_ideas.get(orchestra_id, "")

    async def collect_for_query(
        self,
        query: str,
        original_content: str,
        original_html: str,
        retrieved_docs: List[Any],
        competitor_contents: List[str],
        max_retries: int = 3,
        check_citation_func: Optional[Callable] = None,
    ) -> QueryResultV2:
        """
        ä¸ºå•ä¸ª query æ”¶é›†å»ºè®®ï¼ˆä¸ GEO Agent å®Œå…¨ä¸€è‡´çš„ retry éªŒè¯æœºåˆ¶ï¼‰

        V2.2 æ›´æ–°ï¼ˆä¸ geo_agent å¯¹é½ï¼‰ï¼š
        - å¤ç”¨æˆªæ–­å®¡è®¡ audit_content_truncation()
        - å¤ç”¨ç­–ç•¥å¼•æ“ PolicyEngine å’Œé¥æµ‹å­˜å‚¨ TelemetryStore
        - ä¸¤é˜¶æ®µç­–ç•¥è¯„ä¼°å’Œå·¥å…·è¦†ç›–
        - å¿«é€Ÿå¤±è´¥æ£€æµ‹

        æ ¸å¿ƒé€»è¾‘ï¼š
        1. åˆ›å»ºä¸´æ—¶ HTML ç»“æ„ç”¨äºæµ‹è¯•ä¿®æ”¹
        2. æ¯æ¬¡è¿­ä»£ï¼šå¯¼å‡ºHTML â†’ é‡æ–°è§£æ â†’ è·å–çº¯æ–‡æœ¬ â†’ æ£€æŸ¥å¼•ç”¨
        3. å¤±è´¥åˆ™æ‰§è¡Œå·¥å…·ï¼Œæ›´æ–°ä¸´æ—¶ç»“æ„
        4. æˆåŠŸåˆ™é€€å‡ºï¼Œè¿”å›æœ€ç»ˆå»ºè®®

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹ï¼ˆçº¯æ–‡æœ¬ï¼Œä¸å˜ï¼‰
            original_html: åŸå§‹ HTMLï¼ˆç”¨äºåˆ›å»ºä¸´æ—¶ç»“æ„ï¼‰
            retrieved_docs: æ£€ç´¢åˆ°çš„ç«äº‰æ–‡æ¡£
            competitor_contents: ç«äº‰å¯¹æ‰‹å®Œæ•´å†…å®¹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            check_citation_func: å¼•ç”¨æ£€æŸ¥å‡½æ•°

        Returns:
            QueryResultV2: æŸ¥è¯¢ç»“æœï¼ˆåŒ…å«æœ€ç»ˆç”Ÿæ•ˆçš„å»ºè®®å’Œ final_htmlï¼‰
        """
        import time

        is_cited = False
        generated_answer = ""
        final_diagnosis: Optional[DiagnosisInfo] = None
        final_suggestion: Optional[SuggestionV2] = None
        iterations_used = 0
        # GEO Score ä¿¡æ¯ï¼ˆV2.3 æ–°å¢ï¼‰
        final_geo_score: Optional[GEOScoreInfo] = None

        # ä¸ºè¯¥ query åˆ›å»ºå†…å­˜
        memory = OptimizationMemoryV2()
        self._query_memories[query] = memory

        # ========== åˆå§‹åŒ–é¥æµ‹å­˜å‚¨å’Œç­–ç•¥å¼•æ“ï¼ˆå¤ç”¨ geo_agentï¼‰==========
        telemetry = TelemetryStore(url="", core_idea=self.core_ideas.get(0, ""))
        geo_policy_engine = GeoAgentPolicyEngine(telemetry)

        # ä¿ç•™åŸæœ‰çš„ BatchPolicyEngine ç”¨äºç­–ç•¥æ³¨å…¥ prompt ç”Ÿæˆ
        batch_policy_engine = BatchPolicyEngine(
            self.history_manager,
            memory,
            enable_memory=self.enable_memory,
            enable_history=self.enable_history,
        )

        # ========== åŒç»“æ„ç­–ç•¥ï¼šfrozen_structureï¼ˆç´¢å¼•ç¨³å®šï¼‰+ temp_structureï¼ˆå†…å®¹æœ€æ–°ï¼‰==========
        struct_parser = StructuralHtmlParser(min_length=50)

        # frozen_structure: åªè®¡ç®—ä¸€æ¬¡ chunksï¼Œç”¨äºç¨³å®šçš„ç´¢å¼•æ˜ å°„
        frozen_structure = struct_parser.parse(original_html)
        frozen_structure.calculate_chunks(max_chunk_length=2000)
        frozen_num_chunks = len(frozen_structure._chunks)

        # JS Fallback: å½“ chunks=0 æ—¶ï¼Œåˆ›å»ºè™šæ‹Ÿ chunk åŒ…å«æ•´ä¸ª HTML
        # è¿™æ · static_rendering å·¥å…·å¯ä»¥å°è¯•ä» JS/JSON ä¸­æå–å†…å®¹
        js_fallback_mode = False
        if frozen_num_chunks == 0 and original_html:
            from geo_agent.utils.structural_parser import ContentChunk
            # æ³¨æ„ï¼šä½¿ç”¨ 'id' é”®åï¼ˆä¸æ˜¯ 'geo_id'ï¼‰ä»¥åŒ¹é… apply_modification_to_live çš„æœŸæœ›
            virtual_element = {
                'text_content': original_content if original_content else '',
                'original_html': original_html,
                'id': 'virtual-js-chunk-0',  # ç”¨äºæ ‡è¯†è™šæ‹Ÿ chunk
            }
            virtual_chunk = ContentChunk(index=0, elements=[virtual_element])
            frozen_structure._chunks = [virtual_chunk]
            frozen_num_chunks = 1
            js_fallback_mode = True
            logger.info(f"[JS Fallback] Created virtual chunk from raw HTML ({len(original_html)} chars)")

        # temp_structure: åŠ¨æ€æ›´æ–°çš„ DOMï¼Œç”¨äºå†…å®¹æœ€æ–°çŠ¶æ€
        temp_structure = struct_parser.parse(original_html)
        if js_fallback_mode:
            # åŒæ­¥ temp_structure çš„è™šæ‹Ÿ chunk
            temp_structure._chunks = frozen_structure._chunks.copy()

        # æˆªæ–­å®¡è®¡ä¿¡æ¯ï¼ˆè·¨è¿­ä»£å…±äº«ï¼‰
        truncation_summary: Optional[str] = None
        has_truncation_alert = False

        for iteration in range(max_retries):
            iterations_used = iteration + 1
            tool_start_time = time.time()
            tool_outcome = ToolOutcome.SKIPPED
            tool_error_msg: Optional[str] = None
            args_hash = ""

            logger.info(f"Query '{query[:50]}...' - Iteration {iteration}/{max_retries}")

            try:
                # ========== 1. åˆ·æ–°ç»“æ„ï¼ˆå’Œ GEO Agent å®Œå…¨ä¸€è‡´ï¼‰==========
                current_raw_html = temp_structure.export_html()
                temp_structure = struct_parser.parse(current_raw_html)  # é‡æ–°è§£æ
                temp_content = temp_structure.get_clean_text()  # HTML â†’ çº¯æ–‡æœ¬

                # å¦‚æœè§£æåå†…å®¹ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹å†…å®¹å¹¶è·³è¿‡å¼•ç”¨æ£€æŸ¥
                content_empty = not temp_content or not temp_content.strip()
                if content_empty:
                    logger.warning(f"Empty content after parsing at iteration {iteration}, using original for analysis")
                    temp_content = original_content  # ç”¨äºåç»­æˆªæ–­å®¡è®¡
                    is_cited = False
                    cited_indices = []
                elif check_citation_func:
                    citation_result: CitationCheckResult = await check_citation_func(
                        query, temp_content, retrieved_docs, competitor_contents
                    )
                    is_cited = citation_result.is_cited
                    generated_answer = citation_result.generated_answer
                    cited_indices = citation_result.citations_found_idx

                    # è®¡ç®— GEO Scoreï¼ˆV2.3 æ–°å¢ï¼‰
                    # target_idx é»˜è®¤ä¸º competitor_contents æ•°é‡ + 1ï¼ˆç›®æ ‡æ–‡æ¡£åœ¨æœ€åï¼‰
                    num_sources = len(competitor_contents) + 1
                    target_idx = num_sources  # å‡è®¾ç›®æ ‡æ–‡æ¡£æ˜¯æœ€åä¸€ä¸ª
                    final_geo_score = compute_geo_score(generated_answer, target_idx, num_sources)

                    if is_cited:
                        logger.info(f"âœ… Query '{query[:50]}...' - Cited at iteration {iteration}!")
                        logger.info(f"ğŸ“Š GEO Score: overall={final_geo_score.overall:.4f}, word={final_geo_score.word:.4f}, pos={final_geo_score.position:.4f}")
                        break
                else:
                    cited_indices = []

                # ========== 3. å‡†å¤‡åˆ†ææ‰€éœ€å†…å®¹ ==========
                # ä½¿ç”¨åŒç»“æ„ç­–ç•¥ï¼šç´¢å¼•æ¥è‡ª frozen_structureï¼ˆç¨³å®šï¼‰ï¼Œå†…å®¹æ¥è‡ª temp_structureï¼ˆæœ€æ–°ï¼‰
                indexed_content = frozen_structure.format_indexed_content_with_live_dom(temp_structure)
                num_chunks = frozen_num_chunks  # å§‹ç»ˆä½¿ç”¨å†»ç»“çš„ chunk æ•°é‡

                # ========== 3.1 æˆªæ–­å®¡è®¡ï¼ˆå¤ç”¨ geo_agentï¼‰==========
                if iteration == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ‰§è¡Œæˆªæ–­å®¡è®¡
                    try:
                        audit_res = audit_content_truncation(
                            self.llm,
                            query,
                            full_text=temp_content,
                            visible_chunks_text=indexed_content
                        )
                        if audit_res.has_hidden_relevant_content:
                            logger.info(f"âš ï¸ Truncation Alert: {audit_res.summary_of_hidden_info}")
                            truncation_summary = audit_res.summary_of_hidden_info
                            has_truncation_alert = True
                    except Exception as e:
                        logger.warning(f"Truncation audit failed: {e}")

                # å‡†å¤‡ç«äº‰å¯¹æ‰‹å†…å®¹
                # æ³¨æ„: cited_indices æ˜¯ 1-based (LLM ç”Ÿæˆ [1], [2] ç­‰)
                if cited_indices and competitor_contents:
                    valid_indices = [i for i in cited_indices if 1 <= i <= len(competitor_contents)]
                    if valid_indices:
                        competitor_content = "\n---\n".join(
                            [competitor_contents[i - 1][:3000] for i in valid_indices[:3]]  # i-1 è½¬ä¸º 0-based
                        )
                    else:
                        competitor_content = competitor_contents[0][:3000] if competitor_contents else ""
                else:
                    competitor_content = competitor_contents[0][:3000] if competitor_contents else ""

                # ========== 4. Phase 1 ç­–ç•¥è¯„ä¼°ï¼ˆè¯Šæ–­å‰ï¼ŒåŸºäºæˆªæ–­ä¿¡æ¯ï¼‰==========
                # JS Fallback æ¨¡å¼ï¼šå¼ºåˆ¶ä½¿ç”¨ PARSING_FAILURE è¯Šæ–­
                if js_fallback_mode:
                    pre_diagnosis_category = FailureCategory.PARSING_FAILURE
                    logger.info("[JS Fallback] Using PARSING_FAILURE diagnosis for JS/JSON content")
                elif has_truncation_alert:
                    pre_diagnosis_category = FailureCategory.CONTENT_TRUNCATED
                else:
                    pre_diagnosis_category = FailureCategory.UNKNOWN

                pre_policy_eval = geo_policy_engine.evaluate(
                    diagnosis_category=pre_diagnosis_category,
                    diagnosis_explanation="",
                    has_truncation_alert=has_truncation_alert,
                    hidden_content_summary=truncation_summary or ""
                )

                # ========== 4.1 ç”Ÿæˆç­–ç•¥æ³¨å…¥ï¼ˆåˆå¹¶ geo_agent ç­–ç•¥å’Œ batch ç­–ç•¥ï¼‰==========
                policy_injection = ""
                if self.enable_policy_injection:
                    # ä¼˜å…ˆä½¿ç”¨ geo_agent ç­–ç•¥å¼•æ“çš„æ³¨å…¥
                    if pre_policy_eval.injection_prompt:
                        policy_injection = pre_policy_eval.injection_prompt
                    elif self.enable_memory:
                        # Fallback åˆ° batch_policy_engineï¼ˆä»…å½“ enable_memory=True æ—¶ï¼‰
                        policy_injection = batch_policy_engine.generate_policy_injection(
                            current_diagnosis=final_diagnosis,
                            current_chunk_index=None,
                        )

                # ========== 5. ä¸¤é˜¶æ®µåˆ†æ ==========
                analysis, diagnosis = await analyze_failure_async(
                    llm=self.llm,
                    query=query,
                    indexed_target_doc=indexed_content,
                    competitor_doc=competitor_content,
                    memory=memory,
                    truncation_audit_summary=truncation_summary,  # ä¼ é€’æˆªæ–­ä¿¡æ¯
                    policy_injection=policy_injection,
                    num_chunks=num_chunks,
                    excluded_tools=self.excluded_tools,
                )

                # è®°å½•è¯Šæ–­
                diagnosis_info = diagnosis.to_diagnosis_info()
                final_diagnosis = diagnosis_info
                logger.info(f"Diagnosis: {diagnosis.root_cause} - {diagnosis.key_deficiency}")

                # JS Fallback æ¨¡å¼ï¼šå¼ºåˆ¶ä½¿ç”¨ static_rendering å·¥å…·ï¼ˆä»…ç¬¬ä¸€æ¬¡è¿­ä»£ï¼‰
                if js_fallback_mode and iteration == 0 and "static_rendering" not in self.excluded_tools:
                    original_tool = analysis.selected_tool_name
                    analysis.selected_tool_name = "static_rendering"
                    analysis.tool_arguments = {}  # static_rendering ä¸éœ€è¦é¢å¤–å‚æ•°
                    logger.info(f"[JS Fallback] Overriding tool: {original_tool} -> static_rendering")

                logger.info(f"Tool Selected: {analysis.selected_tool_name}")

                # ========== 5.1 Phase 2 ç­–ç•¥è¯„ä¼°ï¼ˆè¯Šæ–­åï¼‰==========
                diagnosis_category = self._str_to_failure_category(diagnosis.root_cause)

                policy_eval = geo_policy_engine.evaluate(
                    diagnosis_category=diagnosis_category,
                    diagnosis_explanation=diagnosis.explanation,
                    has_truncation_alert=has_truncation_alert,
                    hidden_content_summary=truncation_summary or "",
                    severity=diagnosis.severity
                )

                # ========== 5.2 åº”ç”¨å¼ºåˆ¶å·¥å…·è¦†ç›–ï¼ˆä¿®å¤ï¼šé‡æ–°ç”Ÿæˆå‚æ•°ï¼‰==========
                # æ³¨æ„ï¼šJS Fallback æ¨¡å¼ä¸‹çš„ static_rendering å·¥å…·ä¸åº”è¢«è¦†ç›–
                original_tool = analysis.selected_tool_name
                skip_policy_override = (js_fallback_mode and analysis.selected_tool_name == "static_rendering")
                if policy_eval.forced_tool and policy_eval.forced_tool != analysis.selected_tool_name and not skip_policy_override:
                    logger.info(f"ğŸ¯ Policy Override: {analysis.selected_tool_name} -> {policy_eval.forced_tool}")

                    # é‡æ–°ç”Ÿæˆé€‚é…æ–°å·¥å…·çš„å‚æ•°ï¼ˆä¿®å¤å‚æ•°ä¸åŒ¹é…é—®é¢˜ï¼‰
                    try:
                        history_context = memory.get_history_summary() if self.enable_memory and memory else ""
                        analysis = await regenerate_tool_args_async(
                            llm=self.llm,
                            forced_tool=policy_eval.forced_tool,
                            diagnosis=diagnosis,
                            query=query,
                            target_content_indexed=indexed_content,
                            history_context=history_context,
                            num_chunks=num_chunks,
                        )
                        logger.info(f"âœ… Regenerated args for {policy_eval.forced_tool}")
                    except Exception as e:
                        logger.error(f"Failed to regenerate args for {policy_eval.forced_tool}: {e}")
                        # å›é€€åˆ°åŸå§‹å·¥å…·
                        analysis.selected_tool_name = original_tool
                        logger.warning(f"âš ï¸ Falling back to original tool: {original_tool}")

                # ========== 5.3 æ£€æŸ¥å·¥å…·æ˜¯å¦è¢«ç¦æ­¢ ==========
                if analysis.selected_tool_name in policy_eval.blocked_tools:
                    logger.warning(f"Tool {analysis.selected_tool_name} is blocked by policy, trying next iteration")
                    tool_outcome = ToolOutcome.SKIPPED
                    continue

                # ========== 5.4 å¿«é€Ÿå¤±è´¥æ£€æµ‹ï¼ˆå¤ç”¨ geo_agent çš„åˆ¤æ–­é€»è¾‘ï¼‰==========
                is_fixable, fixable_reason = geo_policy_engine.is_category_fixable(diagnosis_category)
                if not is_fixable and diagnosis.severity == "critical":
                    logger.warning(f"âš¡ Unfixable diagnosis: {diagnosis.root_cause} - {fixable_reason}")
                    break

                if policy_eval.should_skip:
                    logger.warning(f"âš¡ Policy suggests skip: {policy_eval.skip_reason}")
                    if iteration >= 1:  # å·²å°è¯•è‡³å°‘ä¸€æ¬¡
                        break

                # ========== 6. å‡†å¤‡å·¥å…·å‚æ•°ï¼ˆåŒç»“æ„ç­–ç•¥ï¼‰==========
                target_chunk_index = analysis.target_chunk_index or 0
                if target_chunk_index >= num_chunks:
                    target_chunk_index = num_chunks - 1

                # ä½¿ç”¨åŒç»“æ„ç­–ç•¥ï¼šfrozen_structure çš„ç´¢å¼•å®šä½ï¼Œtemp_structure çš„æœ€æ–°å†…å®¹
                tool_args = analysis.tool_arguments.copy()
                tool_args.update(frozen_structure.get_chunk_tool_args_from_live(temp_structure, target_chunk_index))

                # è·å– orchestra çš„æ ¸å¿ƒæ€æƒ³
                orchestra_id = target_chunk_index // self.chunks_per_orchestra
                core_idea = self.core_ideas.get(orchestra_id, "")
                tool_args['core_idea'] = core_idea
                tool_args['previous_modifications'] = memory.get_preservation_rules() if self.enable_memory else ""

                # ä¸º content_relocation å·¥å…·æ·»åŠ å¿…éœ€å‚æ•° (å’Œ geo_agent ä¸€è‡´)
                if analysis.selected_tool_name == "content_relocation":
                    # ä»…å½“ truncation_summary æœ‰å®é™…å†…å®¹æ—¶æ‰è¦†ç›–ï¼Œå¦åˆ™ä¿ç•™ LLM å¯èƒ½ç”Ÿæˆçš„å€¼
                    if truncation_summary:
                        tool_args["hidden_content_summary"] = truncation_summary
                    elif "hidden_content_summary" not in tool_args:
                        tool_args["hidden_content_summary"] = ""
                    tool_args["query"] = query

                # ä¸º intent_realignment å·¥å…·æ·»åŠ å¿…éœ€çš„ user_query å‚æ•°
                if analysis.selected_tool_name == "intent_realignment":
                    tool_args["user_query"] = query

                # ä¸º historical_redteam å·¥å…·æ·»åŠ å¿…éœ€çš„ target_query å‚æ•°
                if analysis.selected_tool_name == "historical_redteam":
                    tool_args["target_query"] = query

                # ========== 6.1 æ£€æŸ¥é‡å¤è°ƒç”¨ï¼ˆä¸ geo_agent å®Œå…¨ä¸€è‡´ï¼‰==========
                args_hash = compute_args_hash(tool_args)
                is_dup, dup_msg = geo_policy_engine.check_duplicate_invocation(analysis.selected_tool_name, args_hash)
                if is_dup:
                    logger.warning(dup_msg)
                    tool_outcome = ToolOutcome.SKIPPED
                    continue

                # ========== 7. æ‰§è¡Œå·¥å…· ==========
                tool = registry.get_tool(analysis.selected_tool_name)
                if not tool:
                    logger.error(f"Tool {analysis.selected_tool_name} not found.")
                    tool_outcome = ToolOutcome.FAILED
                    tool_error_msg = f"Tool {analysis.selected_tool_name} not found"
                    continue

                try:
                    raw_output = await asyncio.to_thread(tool.run, tool_args)
                    modified_chunk_html, key_changes = parse_tool_output(raw_output)
                    tool_outcome = ToolOutcome.SUCCESS

                    # ========== 8. æ›´æ–°ä¸´æ—¶ç»“æ„ï¼ˆåŒç»“æ„ç­–ç•¥ï¼šé€šè¿‡ frozen ç´¢å¼•å®šä½ï¼Œåœ¨ temp ä¸Šä¿®æ”¹ï¼‰==========
                    if js_fallback_mode:
                        # JS Fallback æ¨¡å¼ï¼šè™šæ‹Ÿ chunk æ²¡æœ‰çœŸæ­£çš„ DOM é”šç‚¹
                        # ç›´æ¥ç”¨å·¥å…·è¾“å‡ºçš„æ–° HTML æ›¿æ¢æ•´ä¸ªç»“æ„
                        wrapped_html = f"<html><body>{modified_chunk_html}</body></html>"
                        temp_structure = struct_parser.parse(wrapped_html)
                        temp_structure.calculate_chunks(max_chunk_length=2000)
                        logger.info(f"[JS Fallback] Replaced temp_structure with tool output ({len(modified_chunk_html)} chars)")
                    elif frozen_structure.apply_modification_to_live(temp_structure, target_chunk_index, modified_chunk_html, highlight=False):
                        logger.info(f"DOM updated successfully at frozen chunk index {target_chunk_index}")
                    else:
                        logger.warning(f"Failed to update DOM at frozen chunk index {target_chunk_index}")
                        tool_outcome = ToolOutcome.PARTIAL

                    # åˆ›å»ºå»ºè®®è®°å½•ï¼ˆåŒ…å«æ–°å¢å­—æ®µï¼‰
                    final_suggestion = SuggestionV2(
                        suggestion_id=str(uuid.uuid4())[:8],
                        query=query,
                        tool_name=analysis.selected_tool_name,
                        tool_arguments=analysis.tool_arguments,  # LLM åŸå§‹è¾“å‡º
                        target_segment_index=target_chunk_index,
                        reasoning=analysis.reasoning,
                        proposed_content=modified_chunk_html,
                        key_changes=key_changes,
                        diagnosis=diagnosis_info,
                        iteration=iteration,
                        confidence=self._calculate_confidence(diagnosis_info),
                        executed_arguments=self._sanitize_for_logging(tool_args),  # å®é™…æ‰§è¡Œå‚æ•°ï¼ˆè„±æ•ï¼‰
                        truncation_info={
                            "has_alert": has_truncation_alert,
                            "summary": truncation_summary
                        } if has_truncation_alert else None,
                    )

                    # æ›´æ–°å†…å­˜ï¼ˆä»…å½“ enable_memory=True æ—¶ï¼‰
                    if self.enable_memory:
                        from .memory_manager import ModificationRecordV2
                        record = ModificationRecordV2(
                            query=query,
                            tool_name=analysis.selected_tool_name,
                            reasoning=analysis.reasoning,
                            key_changes=key_changes,
                            diagnosis=diagnosis_info,
                            chunk_index=target_chunk_index,
                        )
                        memory.add_modification(record)

                    logger.info(f"Tool '{analysis.selected_tool_name}' executed, changes: {key_changes}")

                except Exception as e:
                    logger.error(f"Tool execution failed: {e}")
                    import traceback
                    traceback.print_exc()
                    tool_outcome = ToolOutcome.FAILED
                    tool_error_msg = str(e)

                # ========== 9. è®°å½•é¥æµ‹æ•°æ®ï¼ˆä¸ geo_agent å®Œå…¨ä¸€è‡´ï¼‰==========
                tool_duration = (time.time() - tool_start_time) * 1000
                tool_span = ToolInvocationSpan(
                    tool_name=analysis.selected_tool_name,
                    target_chunk_index=target_chunk_index,
                    arguments_hash=args_hash,
                    outcome=tool_outcome,
                    reasoning=analysis.reasoning,
                    duration_ms=tool_duration,
                    error_message=tool_error_msg
                )

                iteration_metrics = IterationMetrics(
                    iteration_index=iteration,
                    query=query,
                    full_doc_length=len(temp_content),
                    visible_chunk_length=len(indexed_content),
                    truncation_ratio=1 - len(indexed_content) / max(len(temp_content), 1),
                    chunk_count=num_chunks,
                    diagnosis_category=diagnosis_category,
                    diagnosis_explanation=diagnosis.explanation,
                    has_hidden_relevant_content=has_truncation_alert,
                    hidden_content_summary=truncation_summary or "",
                    tool_span=tool_span,
                    was_cited=is_cited
                )
                telemetry.record_iteration(iteration_metrics)

            except Exception as e:
                logger.error(f"Analysis failed for query '{query[:50]}...': {e}")
                import traceback
                traceback.print_exc()
                continue

        # è·å–æœ€ç»ˆçš„ HTML
        final_html = temp_structure.export_html() if final_suggestion else None

        return QueryResultV2(
            query=query,
            is_cited=is_cited,
            generated_answer=generated_answer,
            suggestions=[final_suggestion] if final_suggestion else [],
            diagnosis=final_diagnosis,
            iterations_used=iterations_used,
            final_html=final_html,
            # GEO Score å­—æ®µï¼ˆV2.3 æ–°å¢ï¼‰
            geo_score_word=final_geo_score.word if final_geo_score else 0.0,
            geo_score_position=final_geo_score.position if final_geo_score else 0.0,
            geo_score_wordpos=final_geo_score.wordpos if final_geo_score else 0.0,
            geo_score_overall=final_geo_score.overall if final_geo_score else 0.0,
            has_valid_citations=final_geo_score.has_valid_citations if final_geo_score else False,
        )

    def _calculate_confidence(self, diagnosis: DiagnosisInfo) -> float:
        """æ ¹æ®è¯Šæ–­è®¡ç®—ç½®ä¿¡åº¦"""
        severity_scores = {
            "critical": 0.9,
            "high": 0.8,
            "medium": 0.7,
            "low": 0.6,
        }
        return severity_scores.get(diagnosis.severity, 0.7)

    async def collect_batch(
        self,
        queries: List[str],
        current_content: str,
        current_html: str,
        retrieved_docs_func: Callable,
        competitor_contents_func: Callable,
        check_citation_func: Callable,
        max_concurrency: int = 4,
        max_retries_per_query: int = 3,
    ) -> List[QueryResultV2]:
        """
        æ‰¹é‡æ”¶é›†å»ºè®®

        V2.1 æ›´æ–°ï¼š
        - ä¼ é€’ original_html ç»™æ¯ä¸ª query çš„å¤„ç†
        - æ¯ä¸ª query ç‹¬ç«‹åœ¨ä¸´æ—¶ç»“æ„ä¸Šæµ‹è¯•ä¿®æ”¹

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            current_content: å½“å‰å†…å®¹ï¼ˆçº¯æ–‡æœ¬ï¼‰
            current_html: å½“å‰ HTMLï¼ˆå¿…éœ€ï¼Œç”¨äºåˆ›å»ºä¸´æ—¶ç»“æ„ï¼‰
            retrieved_docs_func: è·å–æ£€ç´¢æ–‡æ¡£çš„å‡½æ•° (query) -> List[SearchResult]
            competitor_contents_func: è·å–ç«äº‰å¯¹æ‰‹å†…å®¹çš„å‡½æ•° (docs) -> List[str]
            check_citation_func: æ£€æŸ¥å¼•ç”¨çš„å‡½æ•°
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
            max_retries_per_query: æ¯ä¸ª query çš„æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            List[QueryResultV2]: æŸ¥è¯¢ç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªåŒ…å« final_htmlï¼‰
        """
        semaphore = asyncio.Semaphore(max_concurrency)

        async def process_query(query: str) -> QueryResultV2:
            async with semaphore:
                try:
                    # è·å–æ£€ç´¢æ–‡æ¡£
                    retrieved_docs = await retrieved_docs_func(query)
                    if not retrieved_docs:
                        return QueryResultV2(
                            query=query,
                            is_cited=False,
                            generated_answer="",
                            suggestions=[],
                            error="No retrieved documents",
                        )

                    # è·å–ç«äº‰å¯¹æ‰‹å†…å®¹ï¼ˆè¿”å›è¿‡æ»¤åçš„ docs å’Œ contentsï¼‰
                    result = await competitor_contents_func(retrieved_docs)
                    if isinstance(result, tuple) and len(result) == 2:
                        retrieved_docs, competitor_contents = result
                    else:
                        competitor_contents = result
                    if not competitor_contents:
                        return QueryResultV2(
                            query=query,
                            is_cited=False,
                            generated_answer="",
                            suggestions=[],
                            error="No competitor contents",
                        )

                    # æ”¶é›†å»ºè®®ï¼ˆä¼ é€’ HTMLï¼‰
                    return await self.collect_for_query(
                        query=query,
                        original_content=current_content,
                        original_html=current_html,
                        retrieved_docs=retrieved_docs,
                        competitor_contents=competitor_contents,
                        max_retries=max_retries_per_query,
                        check_citation_func=check_citation_func,
                    )
                except Exception as e:
                    logger.error(f"Failed to process query '{query[:50]}...': {e}")
                    import traceback
                    traceback.print_exc()
                    return QueryResultV2(
                        query=query,
                        is_cited=False,
                        generated_answer="",
                        suggestions=[],
                        error=str(e),
                    )

        tasks = [asyncio.create_task(process_query(q)) for q in queries]
        results = await asyncio.gather(*tasks)

        return results

    def get_all_suggestions(self) -> List[SuggestionV2]:
        """è·å–æ‰€æœ‰æ”¶é›†åˆ°çš„å»ºè®®"""
        all_suggestions = []
        for memory in self._query_memories.values():
            # ä»å†…å­˜ä¸­æå–å»ºè®®
            pass
        return all_suggestions

    def get_diagnosis_stats(self) -> Dict[str, int]:
        """è·å–è¯Šæ–­ç»Ÿè®¡"""
        stats: Dict[str, int] = {}
        for memory in self._query_memories.values():
            for cause, count in memory.diagnosis_stats.items():
                stats[cause] = stats.get(cause, 0) + count
        return stats